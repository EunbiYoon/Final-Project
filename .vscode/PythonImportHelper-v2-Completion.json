[
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "explode",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "split",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "lower",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "trim",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "explode",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "split",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "regexp_replace",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "lower",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "trim",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "fetch_news_api",
        "kind": 2,
        "importPath": "fetch_news",
        "description": "fetch_news",
        "peekOfCode": "def fetch_news_api():\n    ####### Settings ---> Change Variable ######\n    search_keyword = 'apple stock'\n    start_date = \"2024-10-21\"\n    end_date = \"2024-10-22\" \n    sort_method = 'popularity'\n    api_key = \"fcf6368111ce48c3b234b0a479d1dca6\"\n    # url definition\n    url = (\n        \"https://newsapi.org/v2/everything?\"",
        "detail": "fetch_news",
        "documentation": {}
    },
    {
        "label": "fetch_news_about_apple",
        "kind": 2,
        "importPath": "news_text",
        "description": "news_text",
        "peekOfCode": "def fetch_news_about_apple():\n    ####### Settings ---> Change Variable ######\n    search_keyword = 'apple stock'\n    start_date = \"2024-10-21\"\n    end_date = \"2024-10-22\" \n    sort_method = 'popularity'\n    api_key = \"fcf6368111ce48c3b234b0a479d1dca6\"\n    # url definition\n    url = (\n        \"https://newsapi.org/v2/everything?\"",
        "detail": "news_text",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "tranform_pyspark",
        "description": "tranform_pyspark",
        "peekOfCode": "spark = SparkSession.builder.appName(\"TextSentimentAnalysis\").getOrCreate()\n# load the text file\nlog_file=\"news_data.json\"\n#log_data = spark.read.json(log_file).cache()\ndf = spark.read.option(\"multiline\", \"true\").json(log_file)\ndf.printSchema() \ndf.show(truncate=False) \n# Step 3: Check if 'articles' field exists\nif 'articles' in df.columns:\n    print(\"Step 3: 'articles' field exists in the DataFrame\")",
        "detail": "tranform_pyspark",
        "documentation": {}
    },
    {
        "label": "#log_data",
        "kind": 5,
        "importPath": "tranform_pyspark",
        "description": "tranform_pyspark",
        "peekOfCode": "#log_data = spark.read.json(log_file).cache()\ndf = spark.read.option(\"multiline\", \"true\").json(log_file)\ndf.printSchema() \ndf.show(truncate=False) \n# Step 3: Check if 'articles' field exists\nif 'articles' in df.columns:\n    print(\"Step 3: 'articles' field exists in the DataFrame\")\n    # Step 4: Explode the 'articles' field to get individual articles\n    articles_df = df.select(explode(col(\"articles\")).alias(\"article\"))\n    print(\"Step 4: Exploded 'articles' field into individual rows\")",
        "detail": "tranform_pyspark",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "tranform_pyspark",
        "description": "tranform_pyspark",
        "peekOfCode": "df = spark.read.option(\"multiline\", \"true\").json(log_file)\ndf.printSchema() \ndf.show(truncate=False) \n# Step 3: Check if 'articles' field exists\nif 'articles' in df.columns:\n    print(\"Step 3: 'articles' field exists in the DataFrame\")\n    # Step 4: Explode the 'articles' field to get individual articles\n    articles_df = df.select(explode(col(\"articles\")).alias(\"article\"))\n    print(\"Step 4: Exploded 'articles' field into individual rows\")\n    articles_df.printSchema()",
        "detail": "tranform_pyspark",
        "documentation": {}
    },
    {
        "label": "#articles_df",
        "kind": 5,
        "importPath": "tranform_pyspark",
        "description": "tranform_pyspark",
        "peekOfCode": "#articles_df = log_data.select(explode(split(lower(trim(col(\"value\"))), \"\\\\W+\")).alias(\"word\"))\n# extract \n#log_data.show(truncate=False)\nbreakpoint\nspark.stop()",
        "detail": "tranform_pyspark",
        "documentation": {}
    },
    {
        "label": "spark",
        "kind": 5,
        "importPath": "WordFrequency",
        "description": "WordFrequency",
        "peekOfCode": "spark = SparkSession.builder.appName(\"WordFrequency\").getOrCreate()\n# Load the text file\nlogFile = \"news_data.txt\"  \n# Read a text file and change into the Spark DataFrame\nlogData = spark.read.text(logFile).cache()\n# logData.show()\n# +--------------------+\n# |               value|\n# +--------------------+\n# |             HAMLET,|",
        "detail": "WordFrequency",
        "documentation": {}
    },
    {
        "label": "logFile",
        "kind": 5,
        "importPath": "WordFrequency",
        "description": "WordFrequency",
        "peekOfCode": "logFile = \"news_data.txt\"  \n# Read a text file and change into the Spark DataFrame\nlogData = spark.read.text(logFile).cache()\n# logData.show()\n# +--------------------+\n# |               value|\n# +--------------------+\n# |             HAMLET,|\n# |  PRINCE OF DENMARK.|\n# |              ACT I.|",
        "detail": "WordFrequency",
        "documentation": {}
    },
    {
        "label": "logData",
        "kind": 5,
        "importPath": "WordFrequency",
        "description": "WordFrequency",
        "peekOfCode": "logData = spark.read.text(logFile).cache()\n# logData.show()\n# +--------------------+\n# |               value|\n# +--------------------+\n# |             HAMLET,|\n# |  PRINCE OF DENMARK.|\n# |              ACT I.|\n# |Scene I.â€”ELSINORE...|\n# |Francisco on his ...|",
        "detail": "WordFrequency",
        "documentation": {}
    },
    {
        "label": "wordsData_1",
        "kind": 5,
        "importPath": "WordFrequency",
        "description": "WordFrequency",
        "peekOfCode": "wordsData_1 = logData.select(explode(split(lower(trim(col(\"value\"))), \"\\\\W+\")).alias(\"word\"))\n# wordsData_1.show()\n# +---------+\n# |     word|\n# +---------+\n# |   hamlet|\n# |         |\n# |   prince|\n# |       of|\n# |  denmark|",
        "detail": "WordFrequency",
        "documentation": {}
    },
    {
        "label": "wordsData_2",
        "kind": 5,
        "importPath": "WordFrequency",
        "description": "WordFrequency",
        "peekOfCode": "wordsData_2 = wordsData_1.filter(col(\"word\")!=\"\")\n# wordsData_2.show()\n# +---------+\n# |     word|\n# +---------+\n# |   hamlet|\n# |   prince|\n# |       of|\n# |  denmark|\n# |      act|",
        "detail": "WordFrequency",
        "documentation": {}
    },
    {
        "label": "wordCounts",
        "kind": 5,
        "importPath": "WordFrequency",
        "description": "WordFrequency",
        "peekOfCode": "wordCounts = wordsData_2.groupBy('word').count().orderBy(col('count').desc())\n# Show the top 20 most frequent words\nwordCounts.show(20, truncate=False)\n# +----+-----+                                                                    \n# |word|count|\n# +----+-----+\n# |the |1208 |\n# |to  |769  |\n# |and |760  |\n# |i   |728  |",
        "detail": "WordFrequency",
        "documentation": {}
    }
]